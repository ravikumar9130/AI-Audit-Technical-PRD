# Environment Configuration
# Copy this file to .env and update values for your environment

# =============================================================================
# Application Settings
# =============================================================================
ENVIRONMENT=development
LOG_LEVEL=INFO
DEBUG=true

# =============================================================================
# Database (PostgreSQL)
# =============================================================================
DATABASE_URL=postgresql://auditai:auditai@postgres:5432/auditai
DATABASE_POOL_SIZE=20
DATABASE_MAX_OVERFLOW=10

# =============================================================================
# Redis (Broker & Cache)
# =============================================================================
REDIS_URL=redis://redis:6379/0
REDIS_POOL_SIZE=50

# =============================================================================
# MinIO / S3 Object Storage
# =============================================================================
MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=auditai-calls
MINIO_SECURE=false
MINIO_REGION=us-east-1

# AWS S3 Alternative (comment out MinIO settings above if using AWS)
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key
# AWS_REGION=us-east-1
# S3_BUCKET=auditai-calls

# =============================================================================
# JWT Authentication
# =============================================================================
JWT_SECRET=your-super-secret-key-change-in-production-min-32-chars
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_DAYS=7

# =============================================================================
# LLM Configuration (llama.cpp for .gguf; vLLM for HF models)
# =============================================================================
LLM_MODEL_PATH=/app/models/llama-3-8b-instruct-q4.gguf
LLM_MODEL_NAME=llama-3-8b-instruct
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_MAX_MODEL_LEN=4096
VLLM_TENSOR_PARALLEL_SIZE=1
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=2048
# When USE_CPU_LLM=1, max output tokens is capped to avoid 10+ min generation
LLM_MAX_TOKENS_CPU=768
LLM_TOP_P=0.9
# Max transcript chars in prompt (avoids huge context)
TRANSCRIPT_MAX_CHARS=12000

# Alternative: Use smaller model for testing
# LLM_MODEL_PATH=/app/models/mistral-7b-instruct-v0.3-q4.gguf

# =============================================================================
# Audio Processing
# =============================================================================
AUDIO_SAMPLE_RATE=16000
AUDIO_CHANNELS=1
AUDIO_FORMAT=pcm_s16le
VAD_CONFIDENCE_THRESHOLD=0.5
VAD_HANGOVER_MS=250

# =============================================================================
# Security & Compliance
# =============================================================================
ENABLE_AUDIT_LOGGING=true
AUDIT_LOG_RETENTION_DAYS=2555  # 7 years for SOC2
DATA_RETENTION_DAYS=2555
PII_REDACTION_ENABLED=true
ENCRYPTION_KEY=your-encryption-key-for-pii-min-32-chars

# =============================================================================
# Rate Limiting
# =============================================================================
RATE_LIMIT_UPLOADS_PER_MINUTE=10
RATE_LIMIT_API_CALLS_PER_MINUTE=100

# =============================================================================
# WebSocket
# =============================================================================
WS_HEARTBEAT_INTERVAL=30
WS_MAX_CONNECTIONS=1000

# =============================================================================
# Monitoring
# =============================================================================
PROMETHEUS_ENABLED=false
SENTRY_DSN=
